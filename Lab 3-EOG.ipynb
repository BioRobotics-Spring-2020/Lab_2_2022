{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: EOG \n",
    "This notebook goes through processing EOG Signals using NeuroKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once\n",
    "!pip3 install neurokit2\n",
    "!pip3 install mne\n",
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import os\n",
    "plt.rcParams['figure.figsize'] = [15, 9]  # Larger images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyxdf\n",
    "\n",
    "def xdf_to_dataframe(xdf_data):\n",
    "    ''' Xdf Data should be a list of streams (dictionaries)\n",
    "        Function returns a dictionary of dataframes, one dataframe per stream'''  \n",
    "    dataframes = {}\n",
    "    for stream in xdf_data:\n",
    "        df = pd.DataFrame()\n",
    "        data = stream['time_series']\n",
    "        timestamps = stream['time_stamps']\n",
    "        df['Time'] = timestamps\n",
    "        chan_names, units = get_channel_names(stream['info'])\n",
    "        counts = data.shape[0]\n",
    "        for series, name, unit in zip(range(data.shape[1]), chan_names, units):\n",
    "            df[name[0]]  = data[:, series]\n",
    "            if unit:\n",
    "                df[name[0] + '_Unit'] = np.repeat(unit, counts)\n",
    "        \n",
    "        for item in stream['info']:\n",
    "            if item not in ['name', 'desc', 'data']:\n",
    "                try:\n",
    "                    df[item] = np.repeat(stream['info'][item], counts)\n",
    "                except:\n",
    "                    continue\n",
    "        dataframes[stream['info']['name'][0]] = df\n",
    "        \n",
    "    return dataframes\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "def get_channel_names(info):\n",
    "    channels = info['desc'][0]['channels'][0]['channel']\n",
    "    names = [chan['label'] for chan in channels ]\n",
    "    units = [chan['unit'] for chan in channels ]\n",
    "    return names, units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOG Procesing\n",
    "We are first going to read in raw EOG data from one lab member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'lab data/lab 3/lab 3 EOGs'\n",
    "participant= 'PARTICPANT'\n",
    "file = 'FILE'\n",
    "data, header = pyxdf.load_xdf(os.path.join(directory, participant, file))\n",
    "dfs = xdf_to_dataframe(data)\n",
    "display(dfs['BioRadio-20314'])\n",
    "eogdf = dfs['BioRadio-20314']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot the raw signals'''\n",
    "nk.signal_plot(eogdf[['CH1', 'CH2']], sampling_rate=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Plot the raw and clean signals'''\n",
    "eogdf['CH1_Cleaned'] = nk.eog_clean(eogdf['CH1'], sampling_rate=256, method='neurokit')\n",
    "nk.signal_plot([eogdf['CH1'], eogdf['CH1_Cleaned']], labels=[\"Raw Signal\", \"Cleaned Signal\"])\n",
    "plt.figure()\n",
    "eogdf['CH2_Cleaned'] = nk.eog_clean(eogdf['CH2'], sampling_rate=256, method='neurokit')\n",
    "nk.signal_plot([eogdf['CH2'], eogdf['CH2_Cleaned']], labels=[\"Raw Signal\", \"Cleaned Signal\"])\n",
    "plt.figure()\n",
    "nk.signal_plot([eogdf['CH1_Cleaned'], eogdf['CH2_Cleaned']], labels=['CH1', 'CH2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and Find Blinks\n",
    "We can run a peak detection algorithm to find blinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blinks = nk.eog_findpeaks(eogdf['CH1_Cleaned'], sampling_rate=256, method=\"mne\")\n",
    "blinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "We are going to extract features according to this [paper](https://www.sciencedirect.com/science/article/pii/S1877705812012957])\n",
    "\n",
    "Specifically:\n",
    "1. Maximum Peak Value for V and H (CH 1 and 2) \n",
    "- Hint: Max function\n",
    "2. Minimum Peak Value for V and H \n",
    "- Hint: Min function\n",
    "3. Maximum Peak Amplitude Position for V and H\n",
    "- Hint: nk.eog_findpeaks and choose the first non-zero value\n",
    "4. Area under Curve for V and H\n",
    "- Hint: Take the absolute value of each channel and take the sum of the vector ($\\Sigma_{i=1}^N \\vert x \\vert$)\n",
    "5. Variance for V and H\n",
    "- Hint: np.var\n",
    "\n",
    "This should result in 10 features for each sample.\n",
    "\n",
    "## Fill in the 5 feature methods below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataframe, channels):\n",
    "    '''Cycle through each channel'''\n",
    "    feat_dict = {}\n",
    "    for chan in channels:\n",
    "        feat_dict[chan + 'PAV'] = get_max_peak(dataframe[chan])\n",
    "        feat_dict[chan + 'VAV'] = get_min_peak(dataframe[chan])\n",
    "        feat_dict[chan + 'PAP'] = get_peak_amplitude_position(dataframe[chan])\n",
    "        feat_dict[chan + 'AUC'] = get_AUC([chan])\n",
    "        feat_dict[chan + 'VAR'] = get_Variance(dataframe[chan])\n",
    "    return feat_dict\n",
    "        \n",
    "\n",
    "def get_max_peak(signal):\n",
    "    '''Your code here'''\n",
    "    return 1\n",
    "    \n",
    "def get_min_peak(signal):\n",
    "    '''Your code here'''\n",
    "    return 1\n",
    "    \n",
    "def get_peak_amplitude_position(signal):\n",
    "    '''Your Code here'''\n",
    "    return 1\n",
    "    \n",
    "def get_AUC(signal):\n",
    "    '''Your code here'''\n",
    "    return 1\n",
    "    \n",
    "def get_Variance(signal):\n",
    "    '''Your code here'''\n",
    "    return 1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Test your code here'''\n",
    "feats = extract_features(eogdf, ['CH1_Cleaned', 'CH2_Cleaned'])\n",
    "display(feats)\n",
    "'''This should display a dictionary with your extracted features'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Feature Dataframe\n",
    "Now we are going to build our feature dataframe so we can learn off of it\n",
    "\n",
    "You may have to change the process a bit to match your naming conventions. I am assuming you have the following structure:\n",
    "\n",
    "Data -> Participant Data -> .xdf files for each class\n",
    "\n",
    "the .xdf files is assumed to be named CLASSNameTRIAL#.xdf (i.e., lookDown1.xdf, lookDown2.xdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory    = 'lab data/lab 3/lab 3 EOGs'\n",
    "participants = ['P1', 'P2']\n",
    "classes = ['lookDown', 'lookLeft', 'lookRight', 'lookUp']\n",
    "sensor = 'BioRadio-20314'\n",
    "\n",
    "\n",
    "feature_df = pd.DataFrame()\n",
    "\n",
    "'''Cycle through participants, class names, and trials'''\n",
    "for participant in participants:\n",
    "    for cls in classes:\n",
    "        for trial in range(5):\n",
    "            data, header = pyxdf.load_xdf(os.path.join(directory, participant, cls + str(trial + 1) + '.xdf'))\n",
    "            df = xdf_to_dataframe(data)[sensor]\n",
    "            df['CH1'] = nk.eog_clean(df['CH1'], sampling_rate=256, method='neurokit')\n",
    "            df['CH2'] = nk.eog_clean(df['CH2'], sampling_rate=256, method='neurokit')\n",
    "            feats = extract_features(df, ['CH1', 'CH2'])\n",
    "            feats['Participant'] = participant\n",
    "            feats['Class'] = cls\n",
    "            feats['Trial'] = str(trial + 1)\n",
    "            feature_df = pd.concat([feature_df, pd.DataFrame(feats, index=[0])], ignore_index=True)\n",
    "display(feature_df)\n",
    "            \n",
    "'''For three participants, you should have ~60 samples'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Data by Class\n",
    "\n",
    "In the following cells, you should do the following:\n",
    "1. Group by class\n",
    "2. Examine the descripitive statistics for each feature by class\n",
    "3. Examine the correlations by class (look at pandas .corr() method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Machine Learning\n",
    "Now we are going to do some machine learning with SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We are going to do Leave-One-Out Cross Validation'''\n",
    "groups = feature_df['Participant'].values # Specify groups\n",
    "X = feature_df.drop(columns=['Trial', 'Class', 'Participant']).values # specify Feature columns\n",
    "\n",
    "'''Create our Labels from Categories'''\n",
    "le = LabelEncoder()\n",
    "le.fit(feature_df['Class'].values) # Specify Classes\n",
    "y = le.transform(feature_df['Class'].values) # Transform categories into class labels\n",
    "display(y)\n",
    "\n",
    "''' Initialize groups'''\n",
    "logo = LeaveOneGroupOut()\n",
    "'''Look at how many groups we have (should be the number of participants)'''\n",
    "logo.get_n_splits(X, y, groups)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.array([])\n",
    "true = np.array([])\n",
    "\n",
    "'''We can create a classifaction pipeline'''\n",
    "clf = make_pipeline(StandardScaler(), # Standardize the inputs\n",
    "                    PCA(n_components=0.9, svd_solver='full'),  # PCA with 0.9 as the threshold for explain variance\n",
    "                    SVC(kernel='rbf', gamma='auto', class_weight='balanced')) # SVM with RBF kernel\n",
    "\n",
    "''' We can cycle through each group, fit our model, and record our results'''\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    X_train, X_test = X[train_index], X[test_index] # get x training/testing\n",
    "    y_train, y_test = y[train_index], y[test_index] # get y training/testing\n",
    "    clf.fit(X_train, y_train) # fit our model\n",
    "    predicted = np.hstack((predicted, clf.predict(X_test)))\n",
    "    true = np.hstack((true, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "'''View Results'''\n",
    "display(pd.DataFrame(classification_report(true, predicted,output_dict=True)).transpose())\n",
    "\n",
    "cm = confusion_matrix(true, predicted)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "For your lab report, answer the following:\n",
    "\n",
    "1. What features were the most correlated with each other?\n",
    "2. Above we used a SVM with a RBF kernel, what are the accuracies using a polynomial and linear kernel?\n",
    "3. What is a SKLearn pipeline and why is it useful?\n",
    "4. Was there a participant that the model performed worse than the other participants? If so, explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
